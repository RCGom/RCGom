{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvfUjqGmn3KTlOkTFzgKxp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCGom/RCGom/blob/main/BusinessFlows_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai neo4j python-dotenv\n",
        "%pip install neo4j # Ensure neo4j is installed in this environment\n",
        "%pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d_mYSjfJdbPo",
        "outputId": "12290db0-9b02-4282-cde0-a810672592f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.28.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading neo4j-5.28.2-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neo4j\n",
            "Successfully installed neo4j-5.28.2\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.12/dist-packages (5.28.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py THIS is an RAG based on Scenario retrieval, the query adds all related processes\n",
        "# pip install openai neo4j python-dotenv\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "from openai import OpenAI\n",
        "import subprocess\n",
        "\n",
        "import time # Import time for a potential delay\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import gradio as gr\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "3kvYdFHudkyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "OPENAI_API_KEY   = userdata.get(\"OPENAI_API_KEY\")\n",
        "NEO4J_URI        = userdata.get(\"NEO4J_URI\")\n",
        "NEO4J_USER       = userdata.get(\"NEO4J_USER\")\n",
        "NEO4J_PASSWORD   = userdata.get(\"NEO4J_PASSWORD\")\n",
        "NEO4J_DATABASE   = userdata.get(\"NEO4J_DATABASE\")\n",
        "NEO4J_INDEX      = userdata.get(\"NEO4J_INDEX\")\n",
        "CHAT_MODEL = \"gpt-4o-mini\"                  # reasoning + tool use\n",
        "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n"
      ],
      "metadata": {
        "id": "emk5JW-zbPug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SYSTEM_PROMPT = (\n",
        "#     \"You are a process assistant helping users find relevant process definitions. \"\n",
        "#     \"When the user asks a process-related question, call get_processes with a short description of the process\"\n",
        "#     \"the function uses a deep search, not a similarity search.\"\n",
        "#     \"Perform several function calls, if you need different processes to answer the question.\"\n",
        "#     \"Use the returned results to answer and cite the process names you used. \"\n",
        "#     \"If results are poor, say so and ask for clarification.\"\n",
        "# )\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "\n",
        "You are a process assistant. You trying to answer questions based on a process repository\n",
        "which contains End-to-end Scenarios and Business Processes.\n",
        "These can be queried by calling functions\n",
        "- get_scenarios()\n",
        "- get_processes()\n",
        "The function calls provide name, description and related objects of suitable entries.\n",
        "\n",
        "Scenarios have an n:m relation to Processes.\n",
        "\n",
        "1. When the user asks a questions, first interpret the intent\n",
        "   and map it to one or more formal names or descriptions.\n",
        "   - Example: User says \"indirect processing\"\n",
        "     → Rewrite as \"Indirect Procurement Process\"\n",
        "   - Example: User says \"goods receipt\"\n",
        "     → Rewrite as \"Goods Receipt in Inventory Management\"\n",
        "\n",
        "2. Always call the functions - get_scenarios(), get_processes()\n",
        "with the rewritten name(s), not the original vague query.\n",
        "\n",
        "3. If no processes are found (empty results):\n",
        "   - Retry with a broader or parent term (e.g., \"procurement\" if \"indirect\n",
        "     procurement\" fails).\n",
        "   - If still nothing is found, return the most general process in that\n",
        "     domain (e.g., \"Procurement Process\") and tell the user results may not\n",
        "     be exact.\n",
        " please\n",
        "4. Use multiple function calls if several processes are needed.\n",
        "\n",
        "5. Use only the returned results to answer and always cite the process names.\n",
        "\n",
        "6. If results are poor, ask the user to clarify, especially to clarify if a scenario, process or SAP Solution\n",
        "supports answering the question best.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "tzdmMVLohZdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "heeg_1kg4j_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Logging setup\n",
        "# -----------------------------\n",
        "logger = logging.getLogger(\"proc-assistant\")\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "fmt = logging.Formatter(\n",
        "    \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "handler.setFormatter(fmt)\n",
        "logger.addHandler(handler)\n",
        "logger.setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "fPhd42SzgaDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Clients\n",
        "# -----------------------------\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Startup checks\n",
        "# -----------------------------\n",
        "def check_vector_index_exists(index_name: str) -> None:\n",
        "    cypher = \"\"\"\n",
        "    SHOW INDEXES YIELD name, type, entityType, labelsOrTypes, properties\n",
        "    WHERE type = 'VECTOR' AND name = $name\n",
        "    RETURN name, labelsOrTypes, properties\n",
        "    \"\"\"\n",
        "    with driver.session(database=NEO4J_DATABASE) as sess:\n",
        "        rec = sess.run(cypher, name=index_name).single()\n",
        "        if not rec:\n",
        "            logger.warning(\n",
        "                \"Vector index '%s' not found. \"\n",
        "                \"Create it or set NEO4J_INDEX_NAME correctly.\", index_name\n",
        "            )\n",
        "        else:\n",
        "            logger.info(\"Using vector index '%s' on %s %s\",\n",
        "                        rec[\"name\"], rec[\"labelsOrTypes\"], rec[\"properties\"])\n",
        "\n",
        "check_vector_index_exists(NEO4J_INDEX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfZhSREyglBx",
        "outputId": "3ddb8512-3eed-4138-ff3e-89bcdf50ae88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-16 12:03:04 | INFO | proc-assistant | Using vector index 'ProcessDescriptions' on ['Process'] ['embedding']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:proc-assistant:Using vector index 'ProcessDescriptions' on ['Process'] ['embedding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YCNolP7jZKjH"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Embedding + search helpers\n",
        "# -----------------------------\n",
        "# was: EMBEDDING_MODEL = \"text-embedding-3-large\"  # 3072 dims\n",
        "# EMBEDDING_MODEL = \"text-embedding-3-small\"         # 1536 dims\n",
        "\n",
        "\n",
        "def embed(text: str) -> list[float]:\n",
        "    logger.info(\"Embedding model in use: %s\", EMBEDDING_MODEL)\n",
        "    e = client.embeddings.create(model=EMBEDDING_MODEL, input=[text])\n",
        "    vec = e.data[0].embedding\n",
        "    logger.info(\"Embedding length: %d\", len(vec))\n",
        "    return vec\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_search_processes_with_scenarios(search_text: str, limit_processes: int = 6) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Text search over :Process (name, Description) and collect related :Scenario nodes.\n",
        "    Returns:\n",
        "    [\n",
        "      { \"process\": {\"name\": str, \"description\": str},\n",
        "        \"scenarios\": [{\"name\": str, \"description\": str}, ...] },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    cypher = \"\"\"\n",
        "CALL db.index.fulltext.queryNodes('process_ft', $q) YIELD node AS p, score\n",
        "WITH p, score\n",
        "CALL (p) {\n",
        "  MATCH (p)-[:PROCESS_OF]->(s:Scenario)\n",
        "  RETURN collect({\n",
        "    name: s.name,\n",
        "    description: coalesce(s.Description, \"\")\n",
        "  }) AS scenarios\n",
        "}\n",
        "RETURN {\n",
        "  process:  { name: p.name, description: coalesce(p.Description, \"\") },\n",
        "  scenarios: scenarios\n",
        "} AS result\n",
        "ORDER BY score DESC, p.name\n",
        "LIMIT $limit\n",
        "\"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    with driver.session(database=NEO4J_DATABASE) as sess:\n",
        "        rs = sess.run(cypher, q=search_text, limit=limit_processes)\n",
        "        rows = [r[\"result\"] for r in rs]\n",
        "    logger.info(\"Deep search returned %d row(s) in %.1f ms\", len(rows), (time.perf_counter()-t0)*1000)\n",
        "    for r in rows:\n",
        "        logger.info(\"Process=%s | Scenarios=%d\", r[\"process\"][\"name\"], len(r[\"scenarios\"]))\n",
        "    return rows"
      ],
      "metadata": {
        "id": "wfN4PfcX_zcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_search_scenarios_with_processes(search_text: str, limit_processes: int = 5) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Text search over :Scenario (name, Description) and collect related :Process nodes.\n",
        "    Returns:\n",
        "    [\n",
        "      { \"scenario\": {\"name\": str, \"description\": str},\n",
        "        \"processes\": [{\"name\": str, \"description\": str}, ...] },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    cypher = \"\"\"\n",
        "CALL db.index.fulltext.queryNodes('scenario_ft', $q) YIELD node AS s, score\n",
        "WITH s, score\n",
        "CALL (s) {\n",
        "  MATCH (p)-[:PROCESS_OF]->(s:Scenario)\n",
        "  RETURN collect({\n",
        "    name: p.name,\n",
        "    description: coalesce(p.Description, \"\")\n",
        "  }) AS processes\n",
        "}\n",
        "RETURN {\n",
        "  scenario:  { name: s.name, description: coalesce(s.Description, \"\") },\n",
        "  processes: processes\n",
        "} AS result\n",
        "ORDER BY score DESC, s.name\n",
        "LIMIT $limit\n",
        "\"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    with driver.session(database=NEO4J_DATABASE) as sess:\n",
        "        rs = sess.run(cypher, q=search_text, limit=limit_processes)\n",
        "        rows = [r[\"result\"] for r in rs]\n",
        "    logger.info(\"Deep search returned %d row(s) in %.1f ms\", len(rows), (time.perf_counter()-t0)*1000)\n",
        "    for r in rows:\n",
        "        logger.info(\"Process=%s | Scenarios=%d\", r[\"process\"][\"name\"], len(r[\"scenarios\"]))\n",
        "    return rows"
      ],
      "metadata": {
        "id": "6_J4AxYG__eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Tool implementation / processes\n",
        "# -----------------------------\n",
        "def get_processes(query_text: str, k: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Takes a natural-language query, finds up to k matching Processes via\n",
        "    deep text search, and for each returns all connected Scenarios.\n",
        "\n",
        "    Returns a JSON string:\n",
        "    [\n",
        "      {\n",
        "        \"process\":  {\"name\": str, \"description\": str},\n",
        "        \"scenarios\": [{\"name\": str, \"description\": str}, ...]\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    logger.info(\"get_processes (Deep text search) query=%r | k=%d\", query_text, k)\n",
        "\n",
        "    # Run deep text search instead of vector similarity\n",
        "    hits = deep_search_processes_with_scenarios(query_text, limit_processes=k)\n",
        "\n",
        "    # Log concise summary for debugging\n",
        "    logger.info(\"Matched processes: %s\", [h[\"process\"][\"name\"] for h in hits])\n",
        "\n",
        "    # Return JSON for chatbot to use\n",
        "    return json.dumps(hits, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "rv08vv97_qK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Tool implementation / scenarios\n",
        "# -----------------------------\n",
        "def get_scenarios(query_text: str, k: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Takes a natural-language query, finds up to k matching Scenarios via\n",
        "    deep text search, and for each returns all connected Processes.\n",
        "\n",
        "    Returns a JSON string:\n",
        "    [\n",
        "      {\n",
        "        \"scenario\":  {\"name\": str, \"description\": str},\n",
        "        \"processes\": [{\"name\": str, \"description\": str}, ...]\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    logger.info(\"get_scenarios (Deep text search) query=%r | k=%d\", query_text, k)\n",
        "\n",
        "    # Run deep text search instead of vector similarity\n",
        "    hits = deep_search_scenarios_with_processes(query_text, limit_processes=k)\n",
        "\n",
        "    # Log concise summary for debugging\n",
        "    logger.info(\"Matched scenarios: %s\", [h[\"process\"][\"name\"] for h in hits])\n",
        "\n",
        "    # Return JSON for chatbot to use\n",
        "    return json.dumps(hits, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "S9GZzAryC8Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOOLS = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_processes\",\n",
        "        \"description\": \"Return the most similar process definitions by name and description, based on a text query.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query_text\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"User's description of the process they need.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query_text\"],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "},\n",
        "         {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_scenarios\",\n",
        "        \"description\": \"Return the most similar scenario definitions by name and description, based on a text query.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query_text\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"User's description of the process they need.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query_text\"],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "}]"
      ],
      "metadata": {
        "id": "skvBJyoT_ahf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "491eec54"
      },
      "source": [
        "# -----------------------------\n",
        "# Chatbot assistant function\n",
        "# -----------------------------\n",
        "def assistant_reply(user_query: str, history: list[dict] = []) -> str:\n",
        "    \"\"\"\n",
        "    Provides a conversational response to the user query using the OpenAI Chat API,\n",
        "    optionally calling tools to get relevant information from Neo4j.\n",
        "\n",
        "    Args:\n",
        "        user_query: The user's question or request.\n",
        "        history: List of previous message dictionaries in the conversation,\n",
        "                 formatted as [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}, ...].\n",
        "\n",
        "    Returns:\n",
        "        The assistant's reply as a string.\n",
        "    \"\"\"\n",
        "    logger.info(\"assistant_reply received user_query: %r\", user_query)\n",
        "    # Log history carefully to avoid excessive output with large histories\n",
        "    if len(history) < 20: # Log full history if relatively small\n",
        "        logger.info(\"assistant_reply received history: %r\", history)\n",
        "    else: # Log summary if history is longer\n",
        "        logger.info(\"assistant_reply received history length: %d\", len(history))\n",
        "        logger.info(\"assistant_reply first 10 history items: %r\", history[:10])\n",
        "        logger.info(\"assistant_reply last 10 history items: %r\", history[-10:])\n",
        "\n",
        "\n",
        "    # Construct the initial messages list including system prompt and the provided history\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    # Extend messages with the received history (already in the correct role/content dict format from chatbot_interface)\n",
        "    messages.extend(history)\n",
        "    # Append the current user query as the latest user message dictionary\n",
        "    messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "    logger.info(\"Messages prepared for OpenAI API (first call): %r\", messages)\n",
        "\n",
        "\n",
        "    try:\n",
        "        # First API call: Get assistant's response or tool call\n",
        "        logger.info(\"Calling OpenAI chat completion (first call) with model: %s\", CHAT_MODEL)\n",
        "        response = client.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=messages,\n",
        "            tools=TOOLS,\n",
        "            tool_choice=\"auto\",\n",
        "        )\n",
        "        response_message = response.choices[0].message\n",
        "        logger.info(\"OpenAI response (first call) message object: %s\", response_message) # Log the full message object\n",
        "\n",
        "\n",
        "        # Handle tool calls\n",
        "        if response_message.tool_calls:\n",
        "            logger.info(\"Tool calls detected: %s\", response_message.tool_calls)\n",
        "            available_functions = {\n",
        "                \"get_processes\": get_processes,\n",
        "                \"get_scenarios\": get_scenarios,\n",
        "            }\n",
        "\n",
        "            # Append the assistant's message with tool_calls to the messages list\n",
        "            # This message object returned by client.chat.completions.create is usually directly compatible\n",
        "            # with the 'messages' format for the subsequent API call in a tool-using turn.\n",
        "            messages.append(response_message)\n",
        "            logger.info(\"Messages after appending assistant tool_calls message: %r\", messages)\n",
        "\n",
        "            # Execute tool calls and append results\n",
        "            for tool_call in response_message.tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                # Ensure the function name exists in available_functions\n",
        "                function_to_call = available_functions.get(function_name)\n",
        "                if function_to_call:\n",
        "                    try:\n",
        "                        # Parse arguments from the tool call message\n",
        "                        function_args = json.loads(tool_call.function.arguments)\n",
        "                        logger.info(\"Parsed function arguments: %r\", function_args)\n",
        "\n",
        "                        # Call the tool function\n",
        "                        logger.info(\"Calling tool function: %s with args: %r\", function_name, function_args)\n",
        "                        # Ensure tool functions return a string or JSON string as content\n",
        "                        function_response_content = function_to_call(\n",
        "                            query_text=function_args.get(\"query_text\") # Assuming tool functions take query_text\n",
        "                        )\n",
        "                        logger.info(\"Tool function response content (first 500 chars): %s\", function_response_content[:500])\n",
        "\n",
        "                        # Append tool output message to the messages list in the correct format\n",
        "                        # Role must be 'tool', content is the string output, and must include tool_call_id and name\n",
        "                        messages.append(\n",
        "                            {\n",
        "                                \"tool_call_id\": tool_call.id, # Required for tool response messages\n",
        "                                \"role\": \"tool\",\n",
        "                                \"content\": function_response_content, # Content must be a string\n",
        "                                \"name\": function_name # Required for tool response messages\n",
        "                            }\n",
        "                        )\n",
        "                        logger.info(\"Appended tool output message: %r\", messages[-1])\n",
        "\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.error(\"Error decoding function arguments JSON: %s\", tool_call.function.arguments)\n",
        "                        # Append an error message as tool output if args are invalid\n",
        "                        messages.append(\n",
        "                            {\n",
        "                                \"tool_call_id\": tool_call.id,\n",
        "                                \"role\": \"tool\",\n",
        "                                \"content\": f\"Error: Invalid JSON arguments for tool '{function_name}'.\",\n",
        "                                \"name\": function_name\n",
        "                            }\n",
        "                        )\n",
        "                        logger.info(\"Appended tool invalid args error message: %r\", messages[-1])\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(\"Error executing tool '%s': %s\", function_name, e)\n",
        "                        logger.exception(\"Tool execution traceback:\")\n",
        "                        # Append an error message as tool output\n",
        "                        messages.append(\n",
        "                            {\n",
        "                                \"tool_call_id\": tool_call.id,\n",
        "                                \"role\": \"tool\",\n",
        "                                \"content\": f\"Error executing tool '{function_name}': {e}\", # Content as string\n",
        "                                \"name\": function_name\n",
        "                            }\n",
        "                        )\n",
        "                        logger.info(\"Appended tool execution error message: %r\", messages[-1])\n",
        "\n",
        "                else:\n",
        "                    logger.warning(\"Function '%s' called by model not found in available_functions\", function_name)\n",
        "                    # If the model hallucinates a tool call, append a tool message indicating it wasn't found.\n",
        "                    messages.append(\n",
        "                         {\n",
        "                             \"tool_call_id\": tool_call.id,\n",
        "                             \"role\": \"tool\",\n",
        "                             \"content\": f\"Error: Tool '{function_name}' not found.\",\n",
        "                             \"name\": function_name # Still include the name from the model's call\n",
        "                         }\n",
        "                    )\n",
        "                    logger.info(\"Appended 'tool not found' message: %r\", messages[-1])\n",
        "\n",
        "\n",
        "            # Second API call: Get final response after tool execution\n",
        "            logger.info(\"Calling OpenAI chat completion (second call after tools) with model: %s\", CHAT_MODEL)\n",
        "            logger.info(\"Messages prepared for OpenAI API (second call): %r\", messages)\n",
        "\n",
        "            second_response = client.chat.completions.create(\n",
        "                model=CHAT_MODEL,\n",
        "                messages=messages, # Send the updated messages list including assistant tool_calls and tool outputs\n",
        "            )\n",
        "            final_response_message = second_response.choices[0].message\n",
        "            logger.info(\"OpenAI response (second call) message object: %s\", final_response_message) # Log the full message object\n",
        "\n",
        "            # The content of the second response is the final assistant reply\n",
        "            # Ensure the final response content is a string\n",
        "            final_response = final_response_message.content if final_response_message.content is not None else \"\"\n",
        "            logger.info(\"Final assistant response content (after tool): %s\", final_response)\n",
        "            return final_response\n",
        "\n",
        "        else:\n",
        "            # No function call needed, return the initial response content\n",
        "            logger.info(\"No tool calls detected. Returning initial response content.\")\n",
        "            # Ensure the initial response content is a string\n",
        "            initial_response_content = response_message.content if response_message.content is not None else \"\"\n",
        "            logger.info(\"Initial assistant response content: %s\", initial_response_content)\n",
        "            return initial_response_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"An error occurred during OpenAI chat completion:\") # Log the full exception traceback\n",
        "        # Return a user-friendly error message\n",
        "        return f\"An error occurred while processing your request: {e}\" # Include error message for debugging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a4dbd9d"
      },
      "source": [
        "\n",
        "\n",
        "logger = logging.getLogger(\"proc-assistant\") # Get the logger\n",
        "\n",
        "def chatbot_interface(user_query: str, history: list[dict]) -> tuple[list[dict], list[dict]]:\n",
        "    \"\"\"\n",
        "    Wrapper function for assistant_reply to be used with Gradio.\n",
        "    Manages conversation history. Accepts and returns history in OpenAI messages format.\n",
        "    \"\"\"\n",
        "    logger.info(f\"chatbot_interface received history (dict format): {history}\")\n",
        "    logger.info(f\"chatbot_interface received user_query: {user_query}\")\n",
        "\n",
        "    # The history is already in the correct format (list[dict]) due to gr.Chatbot type='messages'\n",
        "\n",
        "    # Get the response from the assistant\n",
        "    try:\n",
        "        # Pass the history directly to assistant_reply (it expects list[dict])\n",
        "        assistant_response_content = assistant_reply(user_query, history)\n",
        "        logger.info(f\"chatbot_interface received assistant_response_content: {assistant_response_content}\")\n",
        "        assistant_message = {\"role\": \"assistant\", \"content\": assistant_response_content}\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error calling assistant_reply from chatbot_interface:\")\n",
        "        assistant_response_content = \"An error occurred while getting the assistant's response. Please check the logs for details.\" # Provide a fallback message\n",
        "        assistant_message = {\"role\": \"assistant\", \"content\": assistant_response_content}\n",
        "\n",
        "\n",
        "    # Append the new interaction (user query and assistant response) to the history\n",
        "    # History is already in list[dict] format, so we append the new messages\n",
        "    updated_history = history + [{\"role\": \"user\", \"content\": user_query}, assistant_message]\n",
        "\n",
        "    logger.info(f\"chatbot_interface updated history (dict format): {updated_history}\")\n",
        "\n",
        "\n",
        "    # Return the updated history for the Chatbot component and the state\n",
        "    # Both need the history in list[dict] format now because gr.Chatbot has type='messages'\n",
        "    return updated_history, updated_history\n",
        "\n",
        "# Create the Gradio interface with history components\n",
        "with gr.Blocks() as iface:\n",
        "    # Use gr.Chatbot with type='messages' to handle history in OpenAI message format\n",
        "    chatbot = gr.Chatbot(label=\"Process Assistant Chatbot\", type='messages')\n",
        "    msg = gr.Textbox(label=\"Ask a question about business processes.\")\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "    # gr.State is used to maintain the history between interactions\n",
        "    # Initialize state as an empty list for history in dict format\n",
        "    state = gr.State([])\n",
        "\n",
        "    # The submit method connects the input (msg, state) to the function (chatbot_interface)\n",
        "    # and updates the chatbot and state with the returned history (now in dict format).\n",
        "    msg.submit(chatbot_interface, inputs=[msg, state], outputs=[chatbot, state])\n",
        "\n",
        "\n",
        "# Note: We won't launch the interface here as per instructions.\n",
        "# The launching will be handled in a subsequent step or cell."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "53f90ef9",
        "outputId": "881cca07-4c7d-401b-8d76-9bc8f7b42ef1"
      },
      "source": [
        "# Launch the Gradio interface\n",
        "iface.launch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://96e723069c8c876022.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://96e723069c8c876022.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}